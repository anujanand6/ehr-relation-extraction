{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import annotations\n",
    "from utils import open_pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDS_PATH = \"../biobert_re/output/test_predictions.txt\"\n",
    "ACTUALS_PATH = \"../biobert_re/dataset/test_labels_rel.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df = pd.read_csv(PREDS_PATH, sep='\\t')\n",
    "actual_dicts = open_pickle(ACTUALS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_actual_labels(actual_dicts):\n",
    "    actual_df_dict = defaultdict(list)\n",
    "    \n",
    "    for i, actual_dict in enumerate(actual_dicts):\n",
    "        actual_df_dict[\"index\"].append(i)\n",
    "        actual_df_dict[\"label\"].append(actual_dict['label'])\n",
    "        actual_df_dict[\"relation\"].append(actual_dict['relation'].name)\n",
    "\n",
    "    df = pd.DataFrame(actual_df_dict)\n",
    "    return df\n",
    "\n",
    "def gen_classification_reports(df):\n",
    "    report = {}\n",
    "    \n",
    "    print(\"-\"*55)\n",
    "    print(\"Overall Classification Report\")\n",
    "    print(classification_report(\n",
    "            df.label.astype(int),\n",
    "            df.prediction.astype(int)))\n",
    "    \n",
    "    overall_report = classification_report(df.label.astype(int), \n",
    "                                           df.prediction.astype(int), \n",
    "                                           output_dict = True)\n",
    "    report[\"overall\"] = overall_report\n",
    "    \n",
    "    for relation in df.relation.unique():\n",
    "        sub_df = df[df.relation==relation]\n",
    "        print(\"-\"*55)\n",
    "        print(\"Classification Report for {} Relation\".format(relation))\n",
    "        print(classification_report(\n",
    "                sub_df.label.astype(int),\n",
    "                sub_df.prediction.astype(int)))\n",
    "        \n",
    "        rel_report = classification_report(sub_df.label.astype(int), \n",
    "                                           sub_df.prediction.astype(int), \n",
    "                                           output_dict = True)\n",
    "        report[relation] = rel_report\n",
    "    \n",
    "    print(\"-\"*55)\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_df = convert_actual_labels(actual_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = actual_df.merge(preds_df, how=\"left\", on=\"index\", suffixes=(\"_actual\", \"_predicted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "Overall Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98      5448\n",
      "           1       0.98      0.99      0.98      8203\n",
      "\n",
      "    accuracy                           0.98     13651\n",
      "   macro avg       0.98      0.98      0.98     13651\n",
      "weighted avg       0.98      0.98      0.98     13651\n",
      "\n",
      "-------------------------------------------------------\n",
      "Classification Report for Dosage-Drug Relation\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       548\n",
      "           1       0.99      0.99      0.99       864\n",
      "\n",
      "    accuracy                           0.99      1412\n",
      "   macro avg       0.99      0.99      0.99      1412\n",
      "weighted avg       0.99      0.99      0.99      1412\n",
      "\n",
      "-------------------------------------------------------\n",
      "Classification Report for Form-Drug Relation\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       797\n",
      "           1       1.00      1.00      1.00      1364\n",
      "\n",
      "    accuracy                           1.00      2161\n",
      "   macro avg       1.00      1.00      1.00      2161\n",
      "weighted avg       1.00      1.00      1.00      2161\n",
      "\n",
      "-------------------------------------------------------\n",
      "Classification Report for Strength-Drug Relation\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00      1278\n",
      "           1       0.99      1.00      1.00      1241\n",
      "\n",
      "    accuracy                           1.00      2519\n",
      "   macro avg       1.00      1.00      1.00      2519\n",
      "weighted avg       1.00      1.00      1.00      2519\n",
      "\n",
      "-------------------------------------------------------\n",
      "Classification Report for Route-Drug Relation\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       751\n",
      "           1       0.99      1.00      1.00      1059\n",
      "\n",
      "    accuracy                           0.99      1810\n",
      "   macro avg       0.99      0.99      0.99      1810\n",
      "weighted avg       0.99      0.99      0.99      1810\n",
      "\n",
      "-------------------------------------------------------\n",
      "Classification Report for Frequency-Drug Relation\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99      1168\n",
      "           1       0.99      1.00      0.99      1152\n",
      "\n",
      "    accuracy                           0.99      2320\n",
      "   macro avg       0.99      0.99      0.99      2320\n",
      "weighted avg       0.99      0.99      0.99      2320\n",
      "\n",
      "-------------------------------------------------------\n",
      "Classification Report for Reason-Drug Relation\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.84      0.83       365\n",
      "           1       0.94      0.94      0.94       987\n",
      "\n",
      "    accuracy                           0.91      1352\n",
      "   macro avg       0.89      0.89      0.89      1352\n",
      "weighted avg       0.91      0.91      0.91      1352\n",
      "\n",
      "-------------------------------------------------------\n",
      "Classification Report for Duration-Drug Relation\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97        62\n",
      "           1       0.99      0.99      0.99       138\n",
      "\n",
      "    accuracy                           0.98       200\n",
      "   macro avg       0.98      0.98      0.98       200\n",
      "weighted avg       0.98      0.98      0.98       200\n",
      "\n",
      "-------------------------------------------------------\n",
      "Classification Report for ADE-Drug Relation\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.87      0.91       479\n",
      "           1       0.96      0.98      0.97      1398\n",
      "\n",
      "    accuracy                           0.96      1877\n",
      "   macro avg       0.95      0.93      0.94      1877\n",
      "weighted avg       0.96      0.96      0.96      1877\n",
      "\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "report = gen_classification_reports(final_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
